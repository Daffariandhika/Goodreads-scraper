# Book-Data-Scraper

A professional Python-based tool for efficiently scraping and analyzing book data from [Goodreads](https://www.goodreads.com). Designed for researchers, developers, and book enthusiasts who need structured and detailed book metadata in bulk.

---

## âœ¨ Overview

This project includes two main Python scripts that work together to collect and extract book information from Goodreads:

* **`url_scraper.py`** â€” Gathers book URLs based on a genre or search query.
* **`books_scraper.py`** â€” Scrapes detailed book data from the URLs collected.

Output is saved in both **JSON** and **CSV** formats for easy integration and analysis.

---

## ğŸŒŸ Features

### `url_scraper.py`

* Scrapes book URLs from a specific genre or search results page.
* Automatically handles pagination.
* Allows configuration for maximum number of URLs (`max_urls`).
* Saves extracted URLs into a Python file (`books_url.py`) for use in the next script.

### `books_scraper.py`

* Parses and extracts detailed book data from URLs.
* Includes retry logic for failed HTTP requests.
* Automatically rate-limits requests to avoid server blocks.
* Outputs structured data to `books.json` and `books.csv`.

---

## âš™ï¸ Prerequisites

Ensure you have **Python 3.10+** installed.
Install required packages:

```bash
pip install pandas requests beautifulsoup4
```

---

## ğŸ“„ Usage Guide

### 1. `url_scraper.py`

Collects Goodreads book URLs based on a search or genre page.

**Setup:**

* Modify the `genre_url` or `search_url` variable inside the script.
* Set `max_urls` to define how many book links to fetch.

**Run the script:**

```bash
python url_scraper.py
```

**Output:**

* Saves URLs to `books_url.py`

---

### 2. `books_scraper.py`

Scrapes detailed data from each book URL gathered earlier.

**Ensure:**

* `books_url.py` (output of the first script) is in the same directory.

**Run the script:**

```bash
python books_scraper.py
```

**Output:**

* `books.json` â€” Raw JSON format.
* `books.csv` â€” Tabular data for spreadsheets or databases.

---

## ğŸ”§ Parameters & Configuration

### `url_scraper.py`

* `genre_url` or `search_url` â€” Goodreads URL to start scraping.
* `max_urls` â€” Maximum number of book URLs to collect.
* `delay` â€” Delay (in seconds) between requests.

### `books_scraper.py`

* Includes:

  * Rate limiting.
  * Automatic retries (up to 3 times per failed request).
  * Output file options (`books.json`, `books.csv`).

---

## âš¡ Troubleshooting

* **No URLs Found:**

  * Double-check the `search_url` or `genre_url` format.
  * Ensure the page has books listed and hasnâ€™t changed its structure.

* **Script Fails to Scrape Data:**

  * Goodreads may be temporarily blocking requests. Try increasing delay.
  * Make sure `books_url.py` contains valid book URLs.

* **Empty Output Files:**

  * Check that URLs collected are reachable.
  * Review console output for errors or skipped entries.

---

## ğŸš€ Future Improvements

* Proxy support for large-scale scraping
* CLI support for dynamic URL input
* Integration with Goodreads APIs (if public access returns)

---

## ğŸ“š License

This project is released under the MIT License.

---

Made with â¤ï¸ for book lovers and data enthusiasts.
